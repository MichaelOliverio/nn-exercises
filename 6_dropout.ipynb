{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P0KxHk9flIju"
   },
   "source": [
    "# Dropout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "u3jg62TZlIjw",
    "outputId": "c6490433-a694-4a90-b024-68a2b02f08c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.callbacks import History \n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UogJLjLEzzWi"
   },
   "source": [
    "## Carichiamo il dataset\n",
    "\n",
    "In questo notebook addestreremo una rete neurale in grado di comprendere se una recensione relativa a un film è positiva o negativa. A questo scopo utilizzeremo [l'IMDB Moview Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/) un dataset di 50.000 recensioni etichettate come positive o negative.\n",
    "<br>\n",
    "Possiamo caricare il dataset utilizzando direttamente Keras, il parametro num_words ci serve per impostare il numero massimo di parole più frequenti da selezionare e di conseguenza il numero di features del nostro modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "i_FPIDKxlIjz",
    "outputId": "2e4baa61-d4b4-48b3-b73d-dd6d870a0c22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di esempi nel train set: 25000\n",
      "Numero di esempi nel test set: 25000\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb \n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=5000)\n",
    "\n",
    "print(\"Numero di esempi nel train set: %d\" % len(X_train))\n",
    "print(\"Numero di esempi nel test set: %d\" % len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84721XTaDgEN"
   },
   "source": [
    "Adesso abbiamo due liste, entrambe contenenti 25.000 osservazioni, per addestramento e test della nostra rete. Le recensioni sono già codificate in delle liste di id, un id identifica la posizione della relativa parola all'interno della lista ordinata delle parole che compaiono più spesso all'interno del nostro corpus di testo, con un offset di 3 posizioni. L'offset è utilizzato perché le prime 3 posizioni sono utilizzate per **boh**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "Gb3m7UVRD8gq",
    "outputId": "57b4c6e9-084d-491d-d96b-146238662e55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La prima recensione del train set contiene 218 parole\n",
      "IDs delle prime 10 parole delal recensione: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]\n"
     ]
    }
   ],
   "source": [
    "print(\"La prima recensione del train set contiene %d parole\" % len(X_train[0]))\n",
    "print(\"IDs delle prime 10 parole delal recensione: %s\" % X_train[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TKh6hP8hExpk"
   },
   "source": [
    "Ad esempio, la seconda parola della prima recensione ha id 14, quindi corrisponde all'11 parola più frequente all'interno del corpus. Possiamo ottenere la posizione di una parola all'interno della lista delle parole più frequenti tramite il dizionario word_index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "w8pwIhJivDRe",
    "outputId": "fc454236-caa1-4d5a-fc7f-597c050e147d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "word_index['love']-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U1HYGB59z3ft"
   },
   "source": [
    "La parola 'love' è la 113esima parola più frequente all'interno del nostro corpus (nota l'offset di 3). Per poter ricostruire una frase partendo dagli IDs dobbiamo conoscere la relazione inversa, cioè a quale parola è associato un determinato id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "46Vsv6KJvmEH",
    "outputId": "5c294d36-a9ef-43d7-d01b-f3865213777a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "reverse_word_index.get(113+3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u9uOX4T4Flqk"
   },
   "source": [
    "Adesso possiamo utilizzare il dizionario con la relazione parola->id per ricostruire una frase, facciamolo con la prima frase all'interno del set di addestramento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "58-VhRZms_9W",
    "outputId": "e74f6279-6921-4353-a700-4b7a1e3cf37b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW: ? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly ? was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little ? that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big ? for the whole film but these children are amazing and should be ? for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was ? with us all\n",
      "\n",
      "\n",
      "SENTIMENT [1=Positive/0=Negative]: 1\n"
     ]
    }
   ],
   "source": [
    "decoded_review = [reverse_word_index.get(i - 3, '?') for i in X_train[0]]\n",
    "decoded_review = ' '.join(decoded_review)\n",
    "\n",
    "print(\"REVIEW: \"+decoded_review)\n",
    "print(\"\\n\")\n",
    "print(\"SENTIMENT [1=Positive/0=Negative]: %d\" % y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MNFccYty1IHs"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "Le frasi nel corpus di testo sono già codificate in numeri, ma hanno una lunghezza differente,  per poterle utilizzare per addestrare la nostra rete neurale abbiamo bisogno di codificarle per ottenere un numero di features consistente. Abbiamo diverse tecniche per farlo, utilizziamo la più semplice: il one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tTz4y2Zl1Lrt"
   },
   "outputs": [],
   "source": [
    "def onehot_encoding(data, size):\n",
    "    onehot = np.zeros((len(data), size))\n",
    "    for i, d in enumerate(data):\n",
    "        onehot[i,d] = 1.\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "otRhAEilHI44"
   },
   "source": [
    "La funzione definita prende in input un corpus di testo e per ogni frase crea un numero di variabili di comodo pari al numero di parole totale del corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JBKExK9k2sUg",
    "outputId": "4b130cf7-d3f6-4a4d-f7d7-c01e150c9287"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = onehot_encoding(X_train, 5000)\n",
    "X_test = onehot_encoding(X_test, 5000)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "584X1UmGG2bq"
   },
   "source": [
    "Potremmo ottenere risultati migliori utilizzando tecniche più complesse ottimizzate per il natural language processing (leggasi **words embedding**) ma per il momento non complichiamoci troppo la vita e passiamo alla creazione del nostro modello."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dpqDHtg1lIj2"
   },
   "source": [
    "## Creiamo la rete neurale\n",
    "\n",
    "Creiamo un modello di rete neurale profonda con ben 5 strati, 1 di input, 3 nascosti e 1 di output. Trattandosi di un problema di classificazione binaria (recension positiva/negativa) come funzione di attivazione dell'ultimo strato dobbiamo utilizzare la funzione e come funzine di costo la binary crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dUqJU2oElIj2"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(5000,)))\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IjOIerLMHewc"
   },
   "source": [
    "l nostro set di addestramento è rappresentato da una matrice sparsa, dato che ogni esempio ha 5000 colonne ma soltanto qualche dozzina di parole, quindi proviamo ad utilizzare la funzione di ottimizzazione **adamax**, che in questi casi dovrebbe portare a risultati migliori dell'adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ZMM_4bhIASX"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adamax', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "avaqFjnAIyx4"
   },
   "source": [
    "Avviamo l'addestramento per soltanto 10 epoche, fidati basteranno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "colab_type": "code",
    "id": "lELf902jIxkM",
    "outputId": "a0f09a84-72f2-4d2d-958c-422987761136"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 4s 147us/step - loss: 0.3701 - acc: 0.8403\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 3s 130us/step - loss: 0.2001 - acc: 0.9250\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 3s 116us/step - loss: 0.1294 - acc: 0.9555\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 3s 116us/step - loss: 0.0598 - acc: 0.9858\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 3s 122us/step - loss: 0.0210 - acc: 0.9963\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 3s 121us/step - loss: 0.0070 - acc: 0.9988\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 3s 123us/step - loss: 0.0024 - acc: 0.9998\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 3s 125us/step - loss: 0.0011 - acc: 0.9999\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 3s 127us/step - loss: 5.6465e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 3s 125us/step - loss: 3.9564e-04 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb30161e10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2oBXUNt3I8X-"
   },
   "source": [
    "Come vedi dopo pochissimo la nostra rete ha ottenuto risultati sul set di addestramento che tendono alla perfezione, questo ci fa pensare a un problema di overfitting. Verifichiamolo valutando il modello sul set di test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "Z-WMWjxl4VvM",
    "outputId": "4dd380d6-da30-4e21-cf3a-ef44fd6249c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 2s 99us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.699159887341708, 0.87224]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4dt96zpvJN2p"
   },
   "source": [
    "Qui le metriche sono decisamente più scarse, si tratta di overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "79WsmAw14eBV"
   },
   "source": [
    "## Applicare il dropout\n",
    "Cerchiamo di contrastare l'overfitting utilizzando una combinazione di regolarizzazione L2 e Dropout. Possiamo utilizzare il Dropout con Keras aggiungendo un'instanza della classe Dropout come fosse un nuovo strato, questa classe necessità di un unico parametro che raprresenta la percentuale di nodi da mantenere attivi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3394
    },
    "colab_type": "code",
    "id": "_8qecs-z3ob1",
    "outputId": "f1f4e839-e7e3-4d5f-e5b7-2be0749d9406"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25000/25000 [==============================] - 5s 207us/step - loss: 12.4596 - acc: 0.5471\n",
      "Epoch 2/100\n",
      "25000/25000 [==============================] - 5s 191us/step - loss: 0.9065 - acc: 0.7119\n",
      "Epoch 3/100\n",
      "25000/25000 [==============================] - 5s 182us/step - loss: 0.7415 - acc: 0.7999\n",
      "Epoch 4/100\n",
      "25000/25000 [==============================] - 4s 173us/step - loss: 0.7077 - acc: 0.8244\n",
      "Epoch 5/100\n",
      "25000/25000 [==============================] - 4s 169us/step - loss: 0.6853 - acc: 0.8330\n",
      "Epoch 6/100\n",
      "25000/25000 [==============================] - 5s 181us/step - loss: 0.6665 - acc: 0.8405\n",
      "Epoch 7/100\n",
      "25000/25000 [==============================] - 4s 153us/step - loss: 0.6505 - acc: 0.8428\n",
      "Epoch 8/100\n",
      "25000/25000 [==============================] - 4s 177us/step - loss: 0.6144 - acc: 0.8459\n",
      "Epoch 9/100\n",
      "25000/25000 [==============================] - 5s 194us/step - loss: 0.6030 - acc: 0.8500\n",
      "Epoch 10/100\n",
      "25000/25000 [==============================] - 5s 189us/step - loss: 0.5819 - acc: 0.8572\n",
      "Epoch 11/100\n",
      "25000/25000 [==============================] - 5s 181us/step - loss: 0.5821 - acc: 0.8577\n",
      "Epoch 12/100\n",
      "25000/25000 [==============================] - 4s 159us/step - loss: 0.5638 - acc: 0.8620\n",
      "Epoch 13/100\n",
      "25000/25000 [==============================] - 4s 158us/step - loss: 0.5698 - acc: 0.8566\n",
      "Epoch 14/100\n",
      "25000/25000 [==============================] - 4s 164us/step - loss: 0.5508 - acc: 0.8646\n",
      "Epoch 15/100\n",
      "25000/25000 [==============================] - 4s 159us/step - loss: 0.5481 - acc: 0.8627\n",
      "Epoch 16/100\n",
      "25000/25000 [==============================] - 4s 158us/step - loss: 0.5383 - acc: 0.8671\n",
      "Epoch 17/100\n",
      "25000/25000 [==============================] - 4s 159us/step - loss: 0.5383 - acc: 0.8658\n",
      "Epoch 18/100\n",
      "25000/25000 [==============================] - 4s 152us/step - loss: 0.5353 - acc: 0.8659\n",
      "Epoch 19/100\n",
      "25000/25000 [==============================] - 4s 150us/step - loss: 0.5397 - acc: 0.8612\n",
      "Epoch 20/100\n",
      "25000/25000 [==============================] - 4s 155us/step - loss: 0.5264 - acc: 0.8678\n",
      "Epoch 21/100\n",
      "25000/25000 [==============================] - 4s 163us/step - loss: 0.5182 - acc: 0.8700\n",
      "Epoch 22/100\n",
      "25000/25000 [==============================] - 4s 156us/step - loss: 0.5168 - acc: 0.8688\n",
      "Epoch 23/100\n",
      "25000/25000 [==============================] - 4s 151us/step - loss: 0.5234 - acc: 0.8678\n",
      "Epoch 24/100\n",
      "25000/25000 [==============================] - 4s 153us/step - loss: 0.5216 - acc: 0.8677\n",
      "Epoch 25/100\n",
      "25000/25000 [==============================] - 4s 158us/step - loss: 0.5113 - acc: 0.8709\n",
      "Epoch 26/100\n",
      "25000/25000 [==============================] - 4s 169us/step - loss: 0.5119 - acc: 0.8701\n",
      "Epoch 27/100\n",
      "25000/25000 [==============================] - 5s 184us/step - loss: 0.5131 - acc: 0.8696\n",
      "Epoch 28/100\n",
      "25000/25000 [==============================] - 6s 239us/step - loss: 0.5095 - acc: 0.8716\n",
      "Epoch 29/100\n",
      "25000/25000 [==============================] - 5s 198us/step - loss: 0.5053 - acc: 0.8698\n",
      "Epoch 30/100\n",
      "25000/25000 [==============================] - 5s 183us/step - loss: 0.5046 - acc: 0.8731\n",
      "Epoch 31/100\n",
      "25000/25000 [==============================] - 5s 180us/step - loss: 0.5037 - acc: 0.8705\n",
      "Epoch 32/100\n",
      "25000/25000 [==============================] - 5s 182us/step - loss: 0.5044 - acc: 0.8709\n",
      "Epoch 33/100\n",
      "25000/25000 [==============================] - 4s 171us/step - loss: 0.5020 - acc: 0.8728\n",
      "Epoch 34/100\n",
      "25000/25000 [==============================] - 4s 156us/step - loss: 0.4989 - acc: 0.8713\n",
      "Epoch 35/100\n",
      "25000/25000 [==============================] - 4s 161us/step - loss: 0.4936 - acc: 0.8735\n",
      "Epoch 36/100\n",
      "25000/25000 [==============================] - 4s 159us/step - loss: 0.4920 - acc: 0.8738\n",
      "Epoch 37/100\n",
      "25000/25000 [==============================] - 4s 159us/step - loss: 0.4915 - acc: 0.8738\n",
      "Epoch 38/100\n",
      "25000/25000 [==============================] - 4s 160us/step - loss: 0.4911 - acc: 0.8762\n",
      "Epoch 39/100\n",
      "25000/25000 [==============================] - 5s 194us/step - loss: 0.4958 - acc: 0.8729\n",
      "Epoch 40/100\n",
      "25000/25000 [==============================] - 5s 192us/step - loss: 0.4956 - acc: 0.8722\n",
      "Epoch 41/100\n",
      "25000/25000 [==============================] - 5s 186us/step - loss: 0.4931 - acc: 0.8739\n",
      "Epoch 42/100\n",
      "25000/25000 [==============================] - 5s 219us/step - loss: 0.4863 - acc: 0.8757\n",
      "Epoch 43/100\n",
      "25000/25000 [==============================] - 5s 182us/step - loss: 0.4900 - acc: 0.8734\n",
      "Epoch 44/100\n",
      "25000/25000 [==============================] - 4s 160us/step - loss: 0.4854 - acc: 0.8781\n",
      "Epoch 45/100\n",
      "25000/25000 [==============================] - 4s 163us/step - loss: 0.4903 - acc: 0.8731\n",
      "Epoch 46/100\n",
      "25000/25000 [==============================] - 4s 159us/step - loss: 0.4815 - acc: 0.8784\n",
      "Epoch 47/100\n",
      "25000/25000 [==============================] - 4s 168us/step - loss: 0.4918 - acc: 0.8728\n",
      "Epoch 48/100\n",
      "25000/25000 [==============================] - 4s 174us/step - loss: 0.4906 - acc: 0.8742\n",
      "Epoch 49/100\n",
      "25000/25000 [==============================] - 4s 170us/step - loss: 0.4885 - acc: 0.8735\n",
      "Epoch 50/100\n",
      "25000/25000 [==============================] - 4s 167us/step - loss: 0.4862 - acc: 0.8739\n",
      "Epoch 51/100\n",
      "25000/25000 [==============================] - 4s 165us/step - loss: 0.4895 - acc: 0.8710\n",
      "Epoch 52/100\n",
      "25000/25000 [==============================] - 4s 164us/step - loss: 0.4771 - acc: 0.8787\n",
      "Epoch 53/100\n",
      "25000/25000 [==============================] - 4s 158us/step - loss: 0.4804 - acc: 0.8769\n",
      "Epoch 54/100\n",
      "25000/25000 [==============================] - 4s 155us/step - loss: 0.4870 - acc: 0.8759\n",
      "Epoch 55/100\n",
      "25000/25000 [==============================] - 4s 162us/step - loss: 0.4820 - acc: 0.8773\n",
      "Epoch 56/100\n",
      "25000/25000 [==============================] - 4s 162us/step - loss: 0.4809 - acc: 0.8770\n",
      "Epoch 57/100\n",
      "25000/25000 [==============================] - 4s 157us/step - loss: 0.4847 - acc: 0.8745\n",
      "Epoch 58/100\n",
      "25000/25000 [==============================] - 4s 157us/step - loss: 0.4777 - acc: 0.8776\n",
      "Epoch 59/100\n",
      "25000/25000 [==============================] - 4s 176us/step - loss: 0.4832 - acc: 0.8764\n",
      "Epoch 60/100\n",
      "25000/25000 [==============================] - 4s 163us/step - loss: 0.4801 - acc: 0.8733\n",
      "Epoch 61/100\n",
      "25000/25000 [==============================] - 4s 160us/step - loss: 0.4766 - acc: 0.8774\n",
      "Epoch 62/100\n",
      "25000/25000 [==============================] - 4s 163us/step - loss: 0.4815 - acc: 0.8750\n",
      "Epoch 63/100\n",
      "25000/25000 [==============================] - 4s 173us/step - loss: 0.4816 - acc: 0.8767\n",
      "Epoch 64/100\n",
      "25000/25000 [==============================] - 4s 160us/step - loss: 0.4743 - acc: 0.8760\n",
      "Epoch 65/100\n",
      "25000/25000 [==============================] - 4s 175us/step - loss: 0.4721 - acc: 0.8802\n",
      "Epoch 66/100\n",
      "25000/25000 [==============================] - 4s 164us/step - loss: 0.4722 - acc: 0.8774\n",
      "Epoch 67/100\n",
      "25000/25000 [==============================] - 4s 155us/step - loss: 0.4789 - acc: 0.8766\n",
      "Epoch 68/100\n",
      "25000/25000 [==============================] - 4s 155us/step - loss: 0.4837 - acc: 0.8744\n",
      "Epoch 69/100\n",
      "25000/25000 [==============================] - 4s 165us/step - loss: 0.4734 - acc: 0.8800\n",
      "Epoch 70/100\n",
      "25000/25000 [==============================] - 4s 160us/step - loss: 0.4697 - acc: 0.8824\n",
      "Epoch 71/100\n",
      "25000/25000 [==============================] - 4s 178us/step - loss: 0.4756 - acc: 0.8772 1s - loss: 0.4\n",
      "Epoch 72/100\n",
      "25000/25000 [==============================] - 5s 184us/step - loss: 0.4795 - acc: 0.8757\n",
      "Epoch 73/100\n",
      "25000/25000 [==============================] - 4s 177us/step - loss: 0.4673 - acc: 0.8835\n",
      "Epoch 74/100\n",
      "25000/25000 [==============================] - 4s 170us/step - loss: 0.4766 - acc: 0.8773\n",
      "Epoch 75/100\n",
      "25000/25000 [==============================] - 4s 166us/step - loss: 0.4720 - acc: 0.8787\n",
      "Epoch 76/100\n",
      "25000/25000 [==============================] - 4s 159us/step - loss: 0.4701 - acc: 0.8817\n",
      "Epoch 77/100\n",
      "25000/25000 [==============================] - 4s 160us/step - loss: 0.4691 - acc: 0.8786\n",
      "Epoch 78/100\n",
      "25000/25000 [==============================] - 4s 163us/step - loss: 0.4734 - acc: 0.8792\n",
      "Epoch 79/100\n",
      "25000/25000 [==============================] - 4s 166us/step - loss: 0.4723 - acc: 0.8788\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 4s 171us/step - loss: 0.4736 - acc: 0.8786\n",
      "Epoch 81/100\n",
      "25000/25000 [==============================] - 5s 180us/step - loss: 0.4686 - acc: 0.8773\n",
      "Epoch 82/100\n",
      "25000/25000 [==============================] - 4s 172us/step - loss: 0.4715 - acc: 0.8796\n",
      "Epoch 83/100\n",
      "25000/25000 [==============================] - 4s 161us/step - loss: 0.4715 - acc: 0.8773\n",
      "Epoch 84/100\n",
      "25000/25000 [==============================] - 4s 159us/step - loss: 0.4690 - acc: 0.8823\n",
      "Epoch 85/100\n",
      "25000/25000 [==============================] - 4s 153us/step - loss: 0.4725 - acc: 0.8782\n",
      "Epoch 86/100\n",
      "25000/25000 [==============================] - 4s 170us/step - loss: 0.4753 - acc: 0.8786\n",
      "Epoch 87/100\n",
      "25000/25000 [==============================] - 4s 167us/step - loss: 0.4680 - acc: 0.8807\n",
      "Epoch 88/100\n",
      "25000/25000 [==============================] - 4s 169us/step - loss: 0.4694 - acc: 0.8791\n",
      "Epoch 89/100\n",
      "25000/25000 [==============================] - 4s 172us/step - loss: 0.4665 - acc: 0.8808\n",
      "Epoch 90/100\n",
      "25000/25000 [==============================] - 4s 164us/step - loss: 0.4650 - acc: 0.8808\n",
      "Epoch 91/100\n",
      "25000/25000 [==============================] - 4s 162us/step - loss: 0.4625 - acc: 0.8822\n",
      "Epoch 92/100\n",
      "25000/25000 [==============================] - 4s 163us/step - loss: 0.4659 - acc: 0.8818\n",
      "Epoch 93/100\n",
      "25000/25000 [==============================] - 4s 179us/step - loss: 0.4693 - acc: 0.8808\n",
      "Epoch 94/100\n",
      "25000/25000 [==============================] - 4s 169us/step - loss: 0.4616 - acc: 0.8818\n",
      "Epoch 95/100\n",
      "25000/25000 [==============================] - 4s 156us/step - loss: 0.4616 - acc: 0.8827\n",
      "Epoch 96/100\n",
      "25000/25000 [==============================] - 4s 164us/step - loss: 0.4604 - acc: 0.8859\n",
      "Epoch 97/100\n",
      "25000/25000 [==============================] - 4s 171us/step - loss: 0.4597 - acc: 0.8856\n",
      "Epoch 98/100\n",
      "25000/25000 [==============================] - 4s 166us/step - loss: 0.4695 - acc: 0.8783\n",
      "Epoch 99/100\n",
      "25000/25000 [==============================] - 4s 180us/step - loss: 0.4658 - acc: 0.8812\n",
      "Epoch 100/100\n",
      "25000/25000 [==============================] - 4s 176us/step - loss: 0.4615 - acc: 0.8843\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb2d426ef0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import Dropout\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(5000,), kernel_regularizer=l2(0.1)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128,activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32,activation='relu',kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(8,activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adamax', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "heyg-uvrJw9h"
   },
   "source": [
    "I risultati dell'addestramento sono più modesti rispetto a prima, ma stavolta non sembrano mostrare segni di overfitting. Verifichiamolo sul set di test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "IqkUIvM-5gjR",
    "outputId": "bbe63448-113b-49f8-cb55-a0d7e6d214e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 4s 153us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4616530714035034, 0.8756]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LfrXpSebKjGB"
   },
   "source": [
    "Questa volta la nostra rete è riuscita a generalizzare su dati sconosciuti il maniera eccellente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7xn8FPOb6qVy"
   },
   "source": [
    " ## Mettiamo la rete neurale all'opera\n",
    "\n",
    "Adesso che abbiamo addestrato e validato la nostra rete, perché non metterla in azione ? Ho scritto questa funzione per prendere una recensione e processarla in modo tale da poterla dare in pasto alla nostra rete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQXmmUdYlIkE"
   },
   "outputs": [],
   "source": [
    "from re import sub\n",
    "\n",
    "def preprocess(review):\n",
    "    \n",
    "    # Rimuoviamo un'eventuale punteggiatura\n",
    "    review = sub(r'[^\\w\\s]','',review) \n",
    "    # Convertiamo tutto in minuscolo\n",
    "    review = review.lower()\n",
    "    # Creiamo un array di parole\n",
    "    review = review.split(\" \")\n",
    "\n",
    "    # Qui dentro metteremo gli IDs\n",
    "    review_array = []\n",
    "\n",
    "    # Iteriamo lungo le parole della recensione\n",
    "    for word in review:\n",
    "        # proseguiamo se la parola si trova all'interno\n",
    "        # della lista di parole del corpus di addestramento\n",
    "        if word in word_index:\n",
    "            # estraiamo l'indice della parola \n",
    "            index = word_index[word] \n",
    "            # Proseguiamo se l'indice è minore o uguale a 5000\n",
    "            # cioè il numero di parole che abbiamo utilizzato\n",
    "            # per l'addestramento\n",
    "            if index <= 5000:\n",
    "                # aggiungiamo l'indice all'array\n",
    "                review_array.append(word_index[word]+3)\n",
    "                \n",
    "    # Eseguiamo il one hot encoding sulla lista di indici\n",
    "    review_array = onehot_encoding([review_array],5000)\n",
    "    \n",
    "    return review_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un valore maggiore dell'output della rete corrisponde ad una recensione maggiormente positiva, scriviamo una semplice funzione per interpretare l'output della rete come il sentiment della recensione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_to_sentiment(y):\n",
    "    \n",
    "    if(prob>0.9): return \"fantastica\"\n",
    "    elif(prob>0.75): return \"ottima\"\n",
    "    elif(prob>0.55): return \"buona\" \n",
    "    elif(prob>0.45): return \"neutrale\"\n",
    "    elif(prob>0.25): return \"negativa\"\n",
    "    elif(prob>0.1): return \"brutta\"\n",
    "    else: return \"pessima\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZxiwK041Lmgo"
   },
   "source": [
    "Testiamo la nostra rete su una recensione che ho preso da internet, relativa a uno dei film più brutti che ho avuto la sciagura di vedere: Paranormal Activity 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ezt_w9bIK7ws",
    "outputId": "dd383bb1-2669-482f-c343-e52347ac57f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW: what a waste of time and cash.. the movie was pointless. with no flow. no questions answered. just a waste. I never review movies but had to share how bad this was..compared to part 1- 2- and 3.... i don't know what else to say other than how misleading the commercial is.. the commercial was cut and spliced with video and audio that didn't even match what happened in the movie... you have been warned. when the movie was over.. people actually Boo'd. hopefully people will spread the word, and save others from throwing their money away. i know die-hard fans will go and give it a shot, but will be disappointed as well. Sinister was better and actually made you jump quite a few times.\n",
      "\n",
      "\n",
      "La recesione è pessima [0.000689]\n"
     ]
    }
   ],
   "source": [
    "review = \"what a waste of time and cash.. the movie was pointless. with no flow. no questions answered. just a waste. I never review movies but had to share how bad this was..compared to part 1- 2- and 3.... i don't know what else to say other than how misleading the commercial is.. the commercial was cut and spliced with video and audio that didn't even match what happened in the movie... you have been warned. when the movie was over.. people actually Boo'd. hopefully people will spread the word, and save others from throwing their money away. i know die-hard fans will go and give it a shot, but will be disappointed as well. Sinister was better and actually made you jump quite a few times.\"\n",
    "x = preprocess(review)\n",
    "y = model.predict(x)[0]\n",
    "print(\"REVIEW: %s\" % review)\n",
    "print(\"\\n\")\n",
    "print(\"La recesione è %s [%.6f]\" % (prob_to_sentiment(y), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ToOeL8HlNfFc"
   },
   "source": [
    "La nostra rete indica che la recensione è (ovviamente) pessima, ma proprio tanto tanto. Proviamo adesso con una recensione che riguarda Avengers: Infinity War."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MIwc5gEjNdOi",
    "outputId": "c8774d3a-81f6-4088-f022-c7d9fd46f57a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW: This movie will blow your mind and break your heart - and make you desperate to go back for more. Brave, brilliant and better than it has any right to be.\n",
      "\n",
      "\n",
      "La recesione è ottima [0.880576]\n"
     ]
    }
   ],
   "source": [
    "review = \"This movie will blow your mind and break your heart - and make you desperate to go back for more. Brave, brilliant and better than it has any right to be.\"\n",
    "x = preprocess(review)\n",
    "y = model.predict(x)\n",
    "\n",
    "print(\"REVIEW: %s\" % review)\n",
    "print(\"\\n\")\n",
    "print(\"La recesione è %s [%.6f]\" % (prob_to_sentiment(y), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H9YsodowN0TO"
   },
   "source": [
    "La rete dice che la recensione è positiva (dai, a chi non è piaciuto questo film ?). Se vuoi divertirti un po' prova a scrivere la tua recensione, deve essere in linua inglese e ovviamente la nostra rete non riesce a comprendere il sarcasmo o frasi ambigue (es. questo film è una bellissima cagata)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UeHe8jKBOY0I"
   },
   "outputs": [],
   "source": [
    "review = input(\"Write your review: \")\n",
    "x = preprocess(review)\n",
    "prob = model.predict(review)\n",
    "\n",
    "print(\"REVIEW: %s\" % review)\n",
    "print(\"\\n\")\n",
    "print(\"La recesione è %s [%.6f]\" % (prob_to_sentiment(prob), prob))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Dropout.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}